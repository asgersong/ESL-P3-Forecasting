{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: Det angivne modul blev ikke fundet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: Det angivne modul blev ikke fundet."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m ENERGY_DATASET_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../dataset/raw/energy_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m WEATHER_DATASET_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../dataset/raw/weather_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Andreas\\anaconda3\\envs\\esl\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     ArrowDtype,\n\u001b[0;32m     52\u001b[0m     Int8Dtype,\n\u001b[0;32m     53\u001b[0m     Int16Dtype,\n\u001b[0;32m     54\u001b[0m     Int32Dtype,\n\u001b[0;32m     55\u001b[0m     Int64Dtype,\n\u001b[0;32m     56\u001b[0m     UInt8Dtype,\n\u001b[0;32m     57\u001b[0m     UInt16Dtype,\n\u001b[0;32m     58\u001b[0m     UInt32Dtype,\n\u001b[0;32m     59\u001b[0m     UInt64Dtype,\n\u001b[0;32m     60\u001b[0m     Float32Dtype,\n\u001b[0;32m     61\u001b[0m     Float64Dtype,\n\u001b[0;32m     62\u001b[0m     CategoricalDtype,\n\u001b[0;32m     63\u001b[0m     PeriodDtype,\n\u001b[0;32m     64\u001b[0m     IntervalDtype,\n\u001b[0;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     66\u001b[0m     StringDtype,\n\u001b[0;32m     67\u001b[0m     BooleanDtype,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     NA,\n\u001b[0;32m     70\u001b[0m     isna,\n\u001b[0;32m     71\u001b[0m     isnull,\n\u001b[0;32m     72\u001b[0m     notna,\n\u001b[0;32m     73\u001b[0m     notnull,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Index,\n\u001b[0;32m     76\u001b[0m     CategoricalIndex,\n\u001b[0;32m     77\u001b[0m     RangeIndex,\n\u001b[0;32m     78\u001b[0m     MultiIndex,\n\u001b[0;32m     79\u001b[0m     IntervalIndex,\n\u001b[0;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     81\u001b[0m     DatetimeIndex,\n\u001b[0;32m     82\u001b[0m     PeriodIndex,\n\u001b[0;32m     83\u001b[0m     IndexSlice,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     NaT,\n\u001b[0;32m     86\u001b[0m     Period,\n\u001b[0;32m     87\u001b[0m     period_range,\n\u001b[0;32m     88\u001b[0m     Timedelta,\n\u001b[0;32m     89\u001b[0m     timedelta_range,\n\u001b[0;32m     90\u001b[0m     Timestamp,\n\u001b[0;32m     91\u001b[0m     date_range,\n\u001b[0;32m     92\u001b[0m     bdate_range,\n\u001b[0;32m     93\u001b[0m     Interval,\n\u001b[0;32m     94\u001b[0m     interval_range,\n\u001b[0;32m     95\u001b[0m     DateOffset,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     to_numeric,\n\u001b[0;32m     98\u001b[0m     to_datetime,\n\u001b[0;32m     99\u001b[0m     to_timedelta,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     Flags,\n\u001b[0;32m    102\u001b[0m     Grouper,\n\u001b[0;32m    103\u001b[0m     factorize,\n\u001b[0;32m    104\u001b[0m     unique,\n\u001b[0;32m    105\u001b[0m     value_counts,\n\u001b[0;32m    106\u001b[0m     NamedAgg,\n\u001b[0;32m    107\u001b[0m     array,\n\u001b[0;32m    108\u001b[0m     Categorical,\n\u001b[0;32m    109\u001b[0m     set_eng_float_format,\n\u001b[0;32m    110\u001b[0m     Series,\n\u001b[0;32m    111\u001b[0m     DataFrame,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\Andreas\\anaconda3\\envs\\esl\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Andreas\\anaconda3\\envs\\esl\\Lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ENERGY_DATASET_PATH = \"../dataset/raw/energy_dataset.csv\"\n",
    "WEATHER_DATASET_PATH = \"../dataset/raw/weather_features.csv\"\n",
    "\n",
    "energy_dataset = pd.read_csv(ENERGY_DATASET_PATH)\n",
    "weather_dataset = pd.read_csv(WEATHER_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination and Drop of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                temp    wind_speed     clouds_all\n",
      "count  178396.000000  178396.00000  178396.000000\n",
      "mean      289.618605       2.47056      25.073292\n",
      "std         8.026199       2.09591      30.774129\n",
      "min       262.240000       0.00000       0.000000\n",
      "25%       283.670000       1.00000       0.000000\n",
      "50%       289.150000       2.00000      20.000000\n",
      "75%       295.150000       4.00000      40.000000\n",
      "max       315.600000     133.00000     100.000000\n"
     ]
    }
   ],
   "source": [
    "weather = weather_dataset.drop(['temp_min', 'temp_max', 'pressure', 'humidity', 'wind_deg', 'rain_1h', 'rain_3h', 'snow_3h', 'weather_id'], axis=1)\n",
    "print(weather.describe())   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        time  fossil fuels combined  windpower  \\\n",
      "0  2015-01-01 00:00:00+01:00                10156.0     6378.0   \n",
      "1  2015-01-01 01:00:00+01:00                10437.0     5890.0   \n",
      "2  2015-01-01 02:00:00+01:00                 9918.0     5461.0   \n",
      "3  2015-01-01 03:00:00+01:00                 8859.0     5238.0   \n",
      "4  2015-01-01 04:00:00+01:00                 8313.0     4935.0   \n",
      "\n",
      "   windpower forecast  solarpower  solarpower forecast  other green energy  \\\n",
      "0              6436.0        49.0                 17.0              3709.0   \n",
      "1              5856.0        50.0                 16.0              3425.0   \n",
      "2              5454.0        50.0                  8.0              3104.0   \n",
      "3              5151.0        50.0                  2.0              2475.0   \n",
      "4              4861.0        42.0                  9.0              2407.0   \n",
      "\n",
      "   total load forecast  total load actual  price day ahead  price actual  \n",
      "0              26118.0            25385.0            50.10         65.41  \n",
      "1              24934.0            24382.0            48.10         64.92  \n",
      "2              23515.0            22734.0            47.33         64.48  \n",
      "3              22642.0            21286.0            42.27         59.32  \n",
      "4              21785.0            20264.0            38.41         56.04  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = energy_dataset\n",
    "\n",
    "# Combine fossil fuel-based generation\n",
    "fossil_fuels = [\n",
    "    \"generation fossil brown coal/lignite\", \"generation fossil coal-derived gas\",\n",
    "    \"generation fossil gas\", \"generation fossil hard coal\", \"generation fossil oil\",\n",
    "    \"generation fossil oil shale\", \"generation fossil peat\"\n",
    "]\n",
    "df[\"fossil fuels combined\"] = df[fossil_fuels].sum(axis=1)\n",
    "\n",
    "# Rename and aggregate renewable sources\n",
    "df[\"windpower\"] = df[[\"generation wind offshore\", \"generation wind onshore\"]].sum(axis=1)\n",
    "df[\"windpower forecast\"] = df[[\"forecast wind offshore eday ahead\", \"forecast wind onshore day ahead\"]].sum(axis=1)\n",
    "df[\"solarpower\"] = df[\"generation solar\"]\n",
    "df[\"solarpower forecast\"] = df[\"forecast solar day ahead\"]\n",
    "\n",
    "# Aggregate other green energy sources (excluding wind and solar)\n",
    "green_energy_sources = [\n",
    "    \"generation biomass\", \"generation geothermal\", \"generation hydro pumped storage aggregated\",\n",
    "    \"generation hydro run-of-river and poundage\", \"generation hydro water reservoir\",\n",
    "    \"generation marine\", \"generation other\", \"generation other renewable\", \"generation waste\"\n",
    "]\n",
    "df[\"other green energy\"] = df[green_energy_sources].sum(axis=1)\n",
    "\n",
    "# Select relevant columns\n",
    "columns_to_keep = [\n",
    "    \"time\", \"fossil fuels combined\", \"windpower\", \"windpower forecast\", \"solarpower\", \"solarpower forecast\",\n",
    "    \"other green energy\", \"total load forecast\", \"total load actual\", \"price day ahead\", \"price actual\"\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Save the processed dataset\n",
    "df.to_csv(\"../dataset/processed/simplified_dataset.csv\", index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "There are columns with missing values, so we should interpolate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                       0\n",
      "fossil fuels combined     18\n",
      "windpower                 21\n",
      "windpower forecast         0\n",
      "solarpower                21\n",
      "solarpower forecast      539\n",
      "other green energy        21\n",
      "total load forecast        0\n",
      "total load actual         36\n",
      "price day ahead            0\n",
      "price actual               0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas\\AppData\\Local\\Temp\\ipykernel_10168\\951567718.py:12: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.interpolate()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set zero values to NaN\n",
    "df = pd.read_csv(\"simplified_dataset.csv\")\n",
    "df = df.replace(0.0, np.nan)\n",
    "\n",
    "# count the number of missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# interpolate using pandas\n",
    "df = df.interpolate()\n",
    "\n",
    "df.to_csv(\"../dataset/processed/interpolated_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split\n",
    "We will split the dataset into training, validation, and test datasets using a 80/10/10 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = pd.read_csv(\"../dataset/processed/interpolated_dataset.csv\")\n",
    "\n",
    "# Calculate lengths for train, validation, and test sets\n",
    "train_len = int(len(df) * 0.8)\n",
    "val_len = int(len(df) * 0.1)\n",
    "\n",
    "# Split the dataset\n",
    "train, val, test = df[:train_len], df[train_len:train_len + val_len], df[train_len + val_len:]\n",
    "\n",
    "# Separate features and target\n",
    "train_x, train_y = train.drop([\"price actual\"], axis=1), train[\"price actual\"]\n",
    "val_x, val_y = val.drop([\"price actual\"], axis=1), val[\"price actual\"]\n",
    "test_x, test_y = test.drop([\"price actual\"], axis=1), test[\"price actual\"]\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(\"../dataset/processed/train\", exist_ok=True)\n",
    "os.makedirs(\"../dataset/processed/val\", exist_ok=True)\n",
    "os.makedirs(\"../dataset/processed/test\", exist_ok=True)\n",
    "\n",
    "# Save the datasets\n",
    "train_x.to_csv(\"../dataset/processed/train/train_x.csv\", index=False)\n",
    "train_y.to_csv(\"../dataset/processed/train/train_y.csv\", index=False)\n",
    "val_x.to_csv(\"../dataset/processed/val/val_x.csv\", index=False)\n",
    "val_y.to_csv(\"../dataset/processed/val/val_y.csv\", index=False)\n",
    "test_x.to_csv(\"../dataset/processed/test/test_x.csv\", index=False)\n",
    "test_y.to_csv(\"../dataset/processed/test/test_y.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
